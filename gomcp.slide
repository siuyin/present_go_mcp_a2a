# AI / LLM Applications with Go
MCP (Model Context Protocol) and A2A (Agent to Agent) protocol


Loh Siu Yin
Engineer, Beyond Broadcast LLP
siuyin@beyondbroadcast.com


## My target environment
* Local inference, preferably without GPU.
* Applications preferably run on user\'s computer to harness an organisation\'s cumulative compute power.
* Go -- for reliable godocs, great reliability and good performance.

## Technologies
* Ollama: https://ollama.com/
* Ollama Go API. go docs: https://pkg.go.dev/github.com/ollama/ollama/api
* LLMs: qwen3:1.7b, qwen3:0.6b, gemma3:1b, gemma3:270m  
  The above are relatively small models that will run on most intel i3 level CPUs and 2GB RAM.

## Hello world
.code -numbers 01_hello/main.go /func main/,/^}/

Lines 14 - 17: configuration from environment variables  
Lines 25, 26 : call ollama for chat completion

## chat.complete()
.code -numbers 01_hello/main.go / complete/,/^}/

Lines 48, 54: takes the streaming ollam chat response, r, and prints it.  
Line 58: the ollama chat call.

## demo
```
go run ./01_hello/

SYS="Provide very verbose responses." go run ./01_hello/

MODEL=qwen3:0.6b go run ./01_hello/
MODEL=qwen3:0.6b THINK=false go run ./01_hello/
MODEL=qwen3:0.6b THINK=aloud go run ./01_hello/
MODEL=qwen3:0.6b THINK=true go run ./01_hello/
ls -l /tmp/j ; cat /tmp/j

```

## Why Go and not Python?

## Hello World with Python and Langchain
.code -numbers 01_hello/main.py /def main/,/complete/

It looks pretty much the same.  
Ingore lines 18-21 for now. I will come back to it.

## demo
```
time uv run ./01_hello/main.py

time go run ./01_hello/


MODEL=qwen3:0.6b uv run ./01_hello/main.py

MODEL=qwen3:0.6b go run ./01_hello/
```

It works pretty much the same.

## Type safety
.code -numbers 01_hello/main.py /# See:/,/change messages/

`uv run mypy --check-untyped-defs ./01_hello/`

What would happen if I run the non-type compliant code?

## 4 reasons I prefer Go over Python

1. Python makes it easy to write type-unchecked code.  
   Type checks are **only** done during compile time using a separate tool, eg mypy  
   They are **not** enforced during run time.

1. Publishing a custom `Go` package, eg. `dftl`, is as simple as pushing to github.  
   A custom `Python` package requires build, registering, and `twining` to PyPI.  
   See: https://packaging.python.org/en/latest/tutorials/packaging-projects/  
   This discourages refactoring code into reusable packages where appropriate.

1. Go applications are packaged as a single static binary,  
   cross-compilable to windows, mac and linux.  
   Python packaging typically requires building a `docker` image or  
   downloading python and `pip install -r requirements.txt`  
   or `uv sync` (requires uv)

1. I much prefer `Go`\'s import syntax over `Python`\'s  
   See next slide.

## Import statements
Go:

.code 01_hello/main.go /import/,/^\)/

Python:

.code 01_hello/main.py /import/,/def main/-1

## Tool Calling
For a Large Language Model to call tools, it needs to have tool calling capabilities.

gemma3 does not, but qwen3 does.

Also smaller models like `qwen3:0.6b` have limited capabilities to parse complex sentences.
Turning on thinking sometimes helps. In some cases over-thinking leads to bad results.

## demo:
`go run ./02_tool/`  
`PROMPT='Please tell me if you have the exquisite iphone 14 within the confines of your organization?' go run ./02_tool/`  
`THINK=aloud PROMPT='Please tell me if you have the exquisite iphone 14 within the confines of your organization?' go run ./02_tool/`  
`PROMPT='product: iphone 14, check stock level' go run ./02_tool/`  
`PROMPT='product: iphone 15, check stock level' go run ./02_tool/`  
`PROMPT='How much is SimpleX' go run ./02_tool/`  
`THINK=true PROMPT='How much is SimpleX' go run ./02_tool/`

With LLMs you need to deal with inherent non-determinism, just as with humans.

## tool calling code:
.code -numbers 02_tool/main.go /messages/,/newChat/

Lines 38-51: The tool definition code is verbose and painful to write.

## Ollama Chat Completion with tool calling
.code -numbers 02_tool/main.go /func.*complete/,/^}/

Line 78: Inspect chat response and vheck if the LLM has called any tools.

Line 83: Tools are actually called here.

## runToolCalls
.code -numbers 02_tool/main.go /func.*runToolCalls/,/^}/

Line 100: the inventory database is called

Line 101 - 103: output is added to the tool response.

## Model Context Protocol

## Model Context Protocol (MCP)

The minimum comprises an MCP **server** and an MCP **client**.  
We are going to write both in Go.

The **server** program can be an executable on your computer's path, or  
a http server that can send server sent events (SSE), otherwise known as streaming http.

The **client** program will use Anthropic's official SDK:  
https://github.com/modelcontextprotocol/go-sdk  
godoc: https://pkg.go.dev/github.com/modelcontextprotocol/go-sdk@v0.3.0/mcp

## MCP Server
.code -numbers 03_mcpserver/main.go /func main/,/^}/

Line 13: Create a new mcp server.  
Lines 15 - 19: Add a tool to the server.  
Line 20: Ignore for now. I'll come back to it.  
Line 22: When the server file is run, communicate using stdio.

## Compiling the mcp server

`go build -o ~/bin/myserver ./03_mcpserver/`

`~/bin` is shorthand for `$HOME/bin`.  
Programs in this folder can be found and run. 

`ls -lh ~/bin/myserver`

## Demo: using the MCP Server

`go run ./04_mcpclient/`

Did we do anything different from:  
`go run ./02_tool/`

The output certainly looks the same!

## MCP Client
.code -numbers 04_mcpclient/main.go /messages/,/newChat/

Line 34: connect to the MCP Server.  
Line 37: get a tool listing from the MCP Server Client Session.  
Line 39: use tools in LLM chat completion.

## compare with local tool definitions
.code -numbers 02_tool/main.go /messages/,/newChat/

## listTools
.code -numbers 04_mcpclient/main.go /func listTool/,/^}/

Line 77: MCP list tools call

Line 82: The tools from MCP are `jsonschema` format. `ollam.FromMCP` converts them to ollama api.Tool format.
