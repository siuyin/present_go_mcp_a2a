# AI / LLM Applications with Go
MCP (Model Context Protocol) and A2A (Agent to Agent) protocol


Loh Siu Yin
Engineer, Beyond Broadcast LLP
siuyin@beyondbroadcast.com


## My target environment
* Local inference, preferably without GPU.
* Applications preferably run on user\'s computer to harness an organisation\'s cumulative compute power.
* Go -- for reliable godocs, great reliability and good performance.

## Technologies
* Ollama: https://ollama.com/
* Ollama Go API. go docs: https://pkg.go.dev/github.com/ollama/ollama/api
* LLMs: qwen3:1.7b, qwen3:0.6b, gemma3:1b, gemma3:270m  
  The above are relatively small models that will run on most intel i3 level CPUs and 2GB RAM.

## Hello world
.code -numbers 01_hello/main.go /func main/,/^}/

Lines 14 - 17: configuration from environment variables  
Lines 25, 26 : call ollama for chat completion

## chat.complete()
.code -numbers 01_hello/main.go / complete/,/^}/

Lines 48, 54: takes the streaming ollam chat response, r, and prints it.  
Line 58: the ollama chat call.

## demo
```
go run ./01_hello/

SYS="Provide very verbose responses." go run ./01_hello/

MODEL=qwen3:0.6b go run ./01_hello/
MODEL=qwen3:0.6b THINK=false go run ./01_hello/
MODEL=qwen3:0.6b THINK=aloud go run ./01_hello/
MODEL=qwen3:0.6b THINK=true go run ./01_hello/
ls -l /tmp/j ; cat /tmp/j

```

## Why Go and not Python?

## Hello World with Python and Langchain
.code -numbers 01_hello/main.py /def main/,/complete/

It looks pretty much the same.  
Ingore lines 18-21 for now. I will come back to it.

## demo
```
time uv run ./01_hello/main.py

time go run ./01_hello/


MODEL=qwen3:0.6b uv run ./01_hello/main.py

MODEL=qwen3:0.6b go run ./01_hello/
```

It works pretty much the same.

## Type safety
.code -numbers 01_hello/main.py /# See:/,/change messages/

`uv run mypy --check-untyped-defs ./01_hello/`

What would happen if I run the non-type compliant code?

## 4 reasons I prefer Go over Python

1. Python makes it easy to write type-unchecked code.  
   Type checks are **only** done during compile time using a separate tool, eg mypy  
   They are **not** enforced during run time.

1. Publishing a custom `Go` package, eg. `dftl`, is as simple as pushing to github.  
   A custom `Python` package requires build, registering, and `twining` to PyPI.  
   See: https://packaging.python.org/en/latest/tutorials/packaging-projects/  
   This discourages refactoring code into reusable packages where appropriate.

1. Go applications are packaged as a single static binary,  
   cross-compilable to windows, mac and linux.  
   Python packaging typically requires building a `docker` image or  
   downloading python and `pip install -r requirements.txt`  
   or `uv sync` (requires uv)

1. I much prefer `Go`\'s import syntax over `Python`\'s  
   See next slide.

## Import statements
Go:

.code 01_hello/main.go /import/,/^\)/

Python:

.code 01_hello/main.py /import/,/def main/-1

## Tool Calling
For a Large Language Model to call tools, it needs to have tool calling capabilities.

gemma3 does not, but qwen3 does.

Also smaller models like `qwen3:0.6b` have limited capabilities to parse complex sentences.
Turning on thinking sometimes helps. In some cases over-thinking leads to bad results.

## demo:
`go run ./02_tool/`  
`PROMPT='Please tell me if you have the exquisite iphone 14 within the confines of your organization?' go run ./02_tool/`  
`THINK=aloud PROMPT='Please tell me if you have the exquisite iphone 14 within the confines of your organization?' go run ./02_tool/`  
`PROMPT='product: iphone 14, check stock level' go run ./02_tool/`  
`PROMPT='product: iphone 15, check stock level' go run ./02_tool/`  
`PROMPT='How much is SimpleX' go run ./02_tool/`  
`THINK=true PROMPT='How much is SimpleX' go run ./02_tool/`

With LLMs you need to deal with inherent non-determinism, just as with humans.

## tool calling code:
.code -numbers 02_tool/main.go /messages/,/newChat/

Lines 38-51: The tool definition code is verbose and painful to write.

## Ollama Chat Completion with tool calling
.code -numbers 02_tool/main.go /func.*complete/,/^}/

Line 78: Inspect chat response and vheck if the LLM has called any tools.

Line 83: Tools are actually called here.

## runToolCalls
.code -numbers 02_tool/main.go /func.*runToolCalls/,/^}/

Line 100: the inventory database is called

Line 101 - 103: output is added to the tool response.

## Model Context Protocol

## Model Context Protocol (MCP)

The minimum comprises an MCP **server** and an MCP **client**.  
We are going to write both in Go.

The **server** program can be an executable on your computer's path, or  
a http server that can send server sent events (SSE), otherwise known as streaming http.

The **client** program will use Anthropic's official SDK:  
https://github.com/modelcontextprotocol/go-sdk  
godoc: https://pkg.go.dev/github.com/modelcontextprotocol/go-sdk@v0.3.0/mcp

## MCP Server
.code -numbers 03_mcpserver/main.go /func main/,/^}/

Line 13: Create a new mcp server.  
Lines 15 - 19: Add a tool to the server.  
Line 20: Ignore for now. I'll come back to it.  
Line 22: When the server file is run, communicate using stdio.

## Compiling the mcp server

`go build -o ~/bin/myserver ./03_mcpserver/`

`~/bin` is shorthand for `$HOME/bin`.  
Programs in this folder can be found and run. 

`ls -lh ~/bin/myserver`

## Demo: using the MCP Server

`go run ./04_mcpclient/`

Did we do anything different from:  
`go run ./02_tool/`

The output certainly looks the same!

## MCP Client
.code -numbers 04_mcpclient/main.go /messages/,/newChat/

Line 34: connect to the MCP Server.  
Line 37: get a tool listing from the MCP Server Client Session.  
Line 39: use tools in LLM chat completion.

## compare with local tool definitions
.code -numbers 02_tool/main.go /messages/,/newChat/

## listTools
.code -numbers 04_mcpclient/main.go /func listTool/,/^}/

Line 77: MCP list tools call

Line 82: The tools from MCP are `jsonschema` format. `ollam.FromMCP` converts them to ollama api.Tool format.

## newChat
.code 04_mcpclient/main.go /func newChat/,/^}/

.code 04_mcpclient/main.go /type myChat/,/^}/

## chat completion
.code 04_mcpclient/main.go /func.*complete/,/^}/

## mcpCallTool
.code 04_mcpclient/main.go /func mcpCallTool/,/^}/

## Recap
1. Model Context Protocol (MCP) makes it easier to build tools that are callable from an LLM's MCP Client.

1. MCP tools may be served over stdio or streaming HTTP (with server sent events).

1. The MCP tools list returned from the MCP Server is in json schema format.
   Conversion to the LLM's local tool calling format may be required.

1. MCP also provides prompts and resources to LLM MCP Clients.  
   We will cover this next.

## MCP Prompts
MCP provides the capability to serve prompts to clients.
See Line 20 below.

.code -numbers 03_mcpserver/main.go /func main/,/^}/

## prompt handler
.code  03_mcpserver/main.go /func promptHandler/,/^}/

The prompt handler is called when the client makes a GetPrompt request.

## Exploring MCP prompts with tests
Test setup:

.code 04_mcpclient/prompt_test.go /func TestMCP/,/t.Run/-1

`go test -v -count=1 ./04_mcpclient/`

## list prompt:
.code 04_mcpclient/prompt_test.go /"ListPrompts"/,/\t}\)/

get prompt:

.code 04_mcpclient/prompt_test.go /"GetPrompt"/,/\t}\)/

## prompt messages:
.code 04_mcpclient/prompt_test.go /"PromptMessages"/,/\t}\)/

## Agent to Agent protocol (A2A)

## Agent to Agent protocol (A2A)
A2A enables collaboration between agents.  
These agents need not be LLM powered.

See: [https://github.com/a2aproject/A2A](https://github.com/a2aproject/A2A)

## A2A inventory lookup agent
.code 05_a2ainventory/main.go /func main/,/^}/

Creating a new A2A server requires an AgentCard and a  
TaskManager configured with a message processor.

## message processor
.code -numbers 05_a2ainventory/main.go /inventoryLookupAgent/,/return.*MessageProcessingResult/+1

Line 27: The actual lookup happens here. The rest is boiler-plate code.

## inventoryLookup
.code  05_a2ainventory/main.go /func inventoryLookup/,/^}/

The code above searches for the product_name in text submitted by the user.  
No LLM or AI is used here.

LLM could be used to parse input text for the product_name.
But this could take longer than a brute-force table scan.

## task manager
.code  05_a2ainventory/main.go /func myTaskManager/,/^}/

The task manager is responsible for maintaining state.  
In this case a memory based task manager is created.

## agent card
.code  05_a2ainventory/main.go /func myAgentCard/,/^}/

This enables an agent to share what skills it has.  
The intent is to populate an agent registry so that 
all teams within an organisation can discover relevant agents.

## A2A product enquiry agent
.code -numbers 06_a2aclient/main.go /func main/,/^}/

Line 21: inventoryLookup connects to the inventory A2A remote agent (server).  
Formats the remote agent's response message as a string.


Line 23: An LLM is asked to format the response in a human friendly way.

Note: This is a general purpose program that happens to call an LLM.  
It can have conditionals and loops.

## demo:

`go run ./06_a2aclient/`  
`PROMPT='What is the price of SimpleX and what is your stock level' go run ./06_a2aclient/`  
`PROMPT='Stock level: iphone 15' go run ./06_a2aclient/`  
`PROMPT='How much is the doggy in the window?' go run ./06_a2aclient/`  

`PROMPT='Stock level: iphone 15' MODEL=gemma3:270m go run ./06_a2aclient/`  
`PROMPT='How much is the doggy in the window?' MODEL=gemma3:270m go run ./06_a2aclient/`  

`PROMPT='How much is the iphone 14 in the window?' MODEL=qwen3:0.6b THINK=false go run ./06_a2aclient/`  
`PROMPT='How much is the iphone 14 in the window?' MODEL=qwen3:0.6b THINK=true go run ./06_a2aclient/`  
`cat /tmp/j`  

## inventoryLookup
.code 06_a2aclient/lookup.go /func inventoryLookup/,/^}/  

Note: the timeout if we can't connect to the agent in time.

## sendMsg
.code 06_a2aclient/lookup.go /func sendMsg/,/^}/  

## recap
* A2A compliant agents are general purpose programs which may contain sequences, conditionals or loops.

* Agents may use LLMs for their language generation, entity extraction (format as json) or summarization capabilities.

* A2A protocol allows for long running processes and human in the loop interaction.  
  For details see: [https://a2a-protocol.org/latest/specification/](https://a2a-protocol.org/latest/specification/)

## Source code

[https://github.com/siuyin/present_go_mcp_a2a](https://github.com/siuyin/present_go_mcp_a2a)
